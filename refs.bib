@article{Waernbaum2012,
abstract = {In this paper, we compare the robustness properties of a matching estimator with a doubly robust estimator. We describe the robustness properties of matching and subclassification estimators by showing how misspecification of the propensity score model can result in the consistent estimation of an average causal effect. The propensity scores are covariate scores, which are a class of functions that removes bias due to all observed covariates. When matching on a parametric model (e.g., a propensity or a prognostic score), the matching estimator is robust to model misspecifications if the misspecified model belongs to the class of covariate scores. The implication is that there are multiple possibilities for the matching estimator in contrast to the doubly robust estimator in which the researcher has two chances to make reliable inference. In simulations, we compare the finite sample properties of the matching estimator with a simple inverse probability weighting estimator and a doubly robust estimator. For the misspecifications in our study, the mean square error of the matching estimator is smaller than the mean square error of both the simple inverse probability weighting estimator and the doubly robust estimators.},
author = {Waernbaum, Ingeborg},
doi = {10.1002/sim.4496},
file = {:Users/dagniel/Projects/ma/refs/waernbaum.pdf:pdf},
isbn = {0277-6715},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Causal effects,Inverse probability weighting,Prognostic score,Propensity score},
number = {15},
pages = {1572--1581},
pmid = {22359267},
title = {{Model misspecification and robustness in causal inference: Comparing matching with doubly robust estimation}},
volume = {31},
year = {2012}
}
@article{Lunceford2004,
author = {Lunceford, Jared K and Lunceford, Jared K and Davidian, Marie and Davidian, Marie},
doi = {10.1002/sim.1903},
file = {:Users/dagniel/Projects/ma/refs/lunceford.pdf:pdf},
journal = {Statistics in Medicine},
keywords = {covariate balance,double robustness,inverse-probability-of-treatment-weighted-estimato},
number = {19},
pages = {2937--2960},
title = {{Stratification and weighting via the propensity score in estimation of causal treatment e ects: a comparative study}},
volume = {2960},
year = {2004}
}
@article{Leacy2014,
abstract = {Propensity and prognostic score methods seek to improve the quality of causal inference in non- randomized or observational studies by replicating the conditions found in a controlled experiment, at least with respect to observed characteristics. Propensity scores model receipt of the treatment of interest; prognostic scores model the potential outcome under a single treatment condition. While the popularity of propensity score methods continues to grow, prognostic score methods and methods combining propensity and prognostic scores have thus far received little attention. To this end, we performed a simulation study that compared subclassification and full matching on a single estimated propensity or prognostic score with three approaches combining estimated propensity and prognostic scores: full matching on a Mahalanobis distance combining the estimated propensity and prognostic scores (FULL-MAHAL); full matching on the estimated prognostic propensity score within propensity score calipers (FULL-PGPPTY); and subclassification on an estimated propensity and prognostic score grid with 5 × 5 subclasses (SUBCLASS(5*5)). We considered settings in which one, both or neither score model was misspecified. The data generating mechanisms varied in the degree of linearity and additivity in the true treatment assignment and outcome models. FULL-MAHAL and FULL-PGPPTY exhibited strong to superior performance in root mean square error terms across all simulation settings and scenarios. Methods combining propensity and prognostic scores were no less robust to model misspecification than single-score methods even when both score models were incorrectly specified. Our findings support the joint use of propensity and prognostic scores in estimation of the average treatment effect on the treated.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Leacy, Finbarr P. and Stuart, Elizabeth A.},
doi = {10.3851/IMP2701.Changes},
eprint = {NIHMS150003},
file = {:Users/dagniel/Projects/ma/refs/leacy{\_}stuart.pdf:pdf},
isbn = {0000000000000},
issn = {00029378},
journal = {Statistics in medicine},
number = {20},
pages = {161--169},
pmid = {19121661},
title = {{On the joint use of propensity and prognostic scores in estimation of the ATT: A simulation study}},
volume = {33},
year = {2014}
}
@article{Kang2007,
abstract = {When outcomes are missing for reasons beyond an investigator's control, there are two different ways to adjust a parameter estimate for co-variates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the out-come and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorpo-rate them into a weighted or stratified estimate. Doubly robust (DR) proce-dures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly spec-ified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of sim-ple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one.},
archivePrefix = {arXiv},
arxivId = {0804.2973},
author = {Kang, Joseph D. Y. and Schafer, Joseph L.},
doi = {10.1214/07-STS227},
eprint = {0804.2973},
file = {:Users/dagniel/Projects/ma/refs/kang{\_}schafer.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,causal inference,missing data,model-assisted survey estimation,propensity score,weighted estimating equations},
number = {4},
pages = {523--539},
pmid = {18516239},
title = {{Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data}},
url = {http://projecteuclid.org/euclid.ss/1207580167},
volume = {22},
year = {2007}
}
@article{Fan2016,
abstract = {Inverse probability of treatment weighting (IPTW) is a popular method for estimating causal effects in many disciplines. However, empirical studies show that the IPTW estimators can be sensitive to the misspecification of propensity score model. To address this problem, several researchers have proposed new methods to estimate propensity score by directly optimizing the balance of pre-treatment covariates. While these methods appear to empirically perform well, little is known about their theoretical properties. This paper makes two main contributions. First, we conduct a theoretical investigation of one such methodology, the Covariate Balancing Propensity Score (CBPS) recently proposed by Imai and Ratkovic (2014). We characterize the asymptotic bias and efficiency of the CBPS-based IPTW estimator under both arbitrary and local model misspecification as well as correct specification for general balancing functions. Based on this finding, we address an open problem in the literature on how to optimally choose the covariate balancing function for the CBPS methodology. Second, motivated by the form of the optimal covariate balancing function, we further propose a new IPTW estimator by generalizing the CBPS method. We prove that the proposed estimator is consistent if either the propensity score model or the outcome model is correct. In addition to this double robustness property, we also establish that the proposed estimator is semiparametrically efficient when both the propensity score and outcome models are correctly specified. Unlike the standard doubly robust estimators, however, the proposed methodology does not require the estimation of outcome model. To relax the parametric assumptions on the propensity score model and the outcome model, we further consider a sieve estimation approach to estimate the treatment effect. A new “nonparametric double robustness” phenomenon is observed. Our simulations show that the proposed estimator has better finite sample properties than the standard estimators.},
author = {Fan, Jianqing},
file = {:Users/dagniel/Projects/ma/refs/fan{\_}imai.pdf:pdf},
keywords = {average treatment effect,cation,causal inference,dms-1206464 and dms-1406266 and,double robustness,model misspecifi-,nih grants r01-gm072611-12 and,r01-,semiparametric efficiency,sieve estimation,supported by nsf grants},
pages = {1--47},
title = {{Improving Covariate Balancing Propensity Score : A Doubly Robust and Efficient Approach ∗}},
year = {2016}
}
@article{Abadie2012,
abstract = {Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the ?hot deck,? a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials. Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the ?hot deck,? a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials.},
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.1080/01621459.2012.682537},
file = {:Users/dagniel/Projects/ma/refs/A Martingale Representation for Matching Estimators.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hot-deck imputation,martingales,overt bias,treatment effects},
number = {498},
pages = {833--843},
title = {{A Martingale Representation for Matching Estimators}},
url = {http://dx.doi.org/10.1080/01621459.2012.682537},
volume = {107},
year = {2012}
}
@article{Journal2016,
author = {Journal, Scandinavian and Journal, Scandinavian},
file = {:Users/dagniel/Projects/ma/refs/4616334.pdf:pdf},
keywords = {empirical distribution,smoothing},
number = {4},
pages = {501--504},
title = {{Board of the Foundation of the Scandinavian Journal of Statistics Weak Convergence of Smoothed Empirical Processes Author ( s ): Aad van der Vaart Source : Scandinavian Journal of Statistics , Vol . 21 , No . 4 ( Dec ., 1994 ), pp . 501-504 Published by :}},
volume = {21},
year = {2016}
}
@article{Abadie2011,
abstract = {In Abadie and Imbens (2006), it was shown that simple nearest-neighbor matching estimators include a conditional bias term that converges to zero at a rate that may be slower than N1/2. As a result, match- ing estimators are not N1/2-consistent in general. In this article, we propose a bias correction that ren- ders matching estimators N1/2-consistent and asymptotically normal. To demonstrate the methods pro- posed in this article, we apply them to the National Supported Work (NSW) data, originally analyzed in Lalonde (1986). We also carry out a small simulation study based on the NSW example. In this simula- tion study, a simple implementation of the bias-corrected matching estimator performs well compared to both simple matching estimators and to regression estimators in terms of bias, root-mean-squared-error, and coverage rates. Software to compute the estimators proposed in this article is available on the au- thors' web pages (http://www.economics.harvard.edu/faculty/imbens/software.html) and documented in Abadie et al. (2003).},
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.1198/jbes.2009.07333},
file = {:Users/dagniel/Projects/ma/refs/Bias Corrected Matching Estimators for Average Treatment Effects.pdf:pdf},
isbn = {978-90-481-8767-6},
issn = {0735-0015},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {Selection on observables, Treatment effects,selection on observables,treatment effects},
number = {1},
pages = {1--11},
title = {{Bias-Corrected Matching Estimators for Average Treatment Effects}},
volume = {29},
year = {2011}
}
@article{Abadie2016,
abstract = {This is the paper that leads to the correction of the stata code for generating ps scores, for example.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Abadie, Alberto and Imbens, Guido W},
doi = {10.3386/w15301},
eprint = {arXiv:1011.1669v3},
file = {:Users/dagniel/Projects/ma/refs/matching{\_}on{\_}the{\_}estimated{\_}propensity{\_}score.pdf:pdf},
isbn = {9788578110796},
issn = {0012-9682},
journal = {Econometrica},
number = {2},
pages = {781--807},
pmid = {25246403},
title = {{Matching on the estimated propensity score}},
volume = {84},
year = {2016}
}
@article{Abadie2012a,
abstract = {Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the ?hot deck,? a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials. Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the ?hot deck,? a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials.},
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.1080/01621459.2012.682537},
file = {:Users/dagniel/Projects/ma/refs/martingale{\_}matching.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hot-deck imputation,martingales,overt bias,treatment effects},
number = {498},
pages = {833--843},
title = {{A Martingale Representation for Matching Estimators}},
url = {http://dx.doi.org/10.1080/01621459.2012.682537},
volume = {107},
year = {2012}
}
@article{Hjort2003,
abstract = {The traditional use of model selection methods in practice is to proceed as if the final selected model had been chosen in advance, without acknowledging the additional uncertainty introduced by model selection. This often means underreporting of variability and too optimistic confidence intervals. We build a general large-sample likelihood apparatus in which limiting distributions and risk properties of estimators post-selection as well as of model average estimators are precisely described, also explicitly taking modeling bias into account. This allows a drastic reduction in complexity, as competing model averaging schemes may be developed, discussed, and compared inside a statistical prototype experiment where only a few crucial quantities matter. In particular, we offer a frequentist view on Bayesian model averaging methods and give a link to generalized ridge estimators. Our work also leads to new model selection criteria. The methods are illustrated with real data applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hjort, Nils Lid and Claeskens, Gerda},
doi = {10.1198/016214503000000828},
eprint = {arXiv:1011.1669v3},
file = {:Users/dagniel/Projects/ma/refs/Frequentist Model Average Estimators.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {1,an impressive range of,been de-,bias and variance balance,growing models,introduction and summary,likelihood inference,model average estimators,model information criteria,model selection criteria has,moderate misspeci cation},
number = {464},
pages = {879--899},
pmid = {25246403},
title = {{Frequentist Model Average Estimators}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214503000000828},
volume = {98},
year = {2003}
}
@article{Model,
author = {Model, B},
file = {:Users/dagniel/Projects/ma/refs/modelselection.pdf:pdf},
title = {{11 Model selection}}
}
@article{Hjort2003a,
abstract = {The traditional use of model selection methods in practice is to proceed as if the final selected model had been chosen in advance, without acknowledging the additional uncertainty introduced by model selection. This often means underreporting of variability and too optimistic confidence intervals. We build a general large-sample likelihood apparatus in which limiting distributions and risk properties of estimators post-selection as well as of model average estimators are precisely described, also explicitly taking modeling bias into account. This allows a drastic reduction in complexity, as competing model averaging schemes may be developed, discussed, and compared inside a statistical prototype experiment where only a few crucial quantities matter. In particular, we offer a frequentist view on Bayesian model averaging methods and give a link to generalized ridge estimators. Our work also leads to new model selection criteria. The methods are illustrated with real data applications.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hjort, Nils Lid and Claeskens, Gerda},
doi = {10.1198/016214503000000828},
eprint = {arXiv:1011.1669v3},
file = {:Users/dagniel/Projects/ma/refs/Frequentist Model Average Estimators.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
number = {464},
pages = {879--899},
pmid = {25246403},
title = {{Frequentist Model Average Estimators}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214503000000828},
volume = {98},
year = {2003}
}
@article{Wang2013,
author = {Wang, Haiying and Zhou, Sherry Z. F.},
doi = {10.1080/03610926.2011.647218},
file = {:Users/dagniel/Projects/ma/refs/Interval Estimation by Frequentist Model Averaging.pdf:pdf},
issn = {0361-0926},
journal = {Communications in Statistics - Theory and Methods},
keywords = {asymptotic equivalence,confidence interval,model averaging,parametric model,semi-parametric model},
number = {23},
pages = {4342--4356},
title = {{Interval Estimation by Frequentist Model Averaging}},
url = {http://www.tandfonline.com/doi/abs/10.1080/03610926.2011.647218},
volume = {42},
year = {2013}
}
@article{Liu2015,
abstract = {This paper derives the limiting distributions of least squares averaging estimators for linear regression models in a local asymptotic framework. We show that the averaging estimators with fixed weights are asymptotically normal and then develop a plug-in averaging estimator that minimizes the sample analog of the asymptotic mean squared error. We investigate the focused information criterion (Claeskens and Hjort, 2003), the plug-in averaging estimator, the Mallows model averaging estimator (Hansen, 2007), and the jackknife model averaging estimator (Hansen and Racine, 2012). We find that the asymptotic distributions of averaging estimators with data-dependent weights are nonstandard and cannot be approximated by simulation. To address this issue, we propose a simple procedure to construct valid confidence intervals with improved coverage probability. Monte Carlo simulations show that the plug-in averaging estimator generally has smaller expected squared error than other existing model averaging methods, and the coverage probability of proposed confidence intervals achieves the nominal level. As an empirical illustration, the proposed methodology is applied to cross-country growth regressions.},
author = {Liu, Chu An},
doi = {10.1016/j.jeconom.2014.07.002},
file = {:Users/dagniel/Projects/ma/refs/MPRA{\_}paper{\_}54201.pdf:pdf},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Local asymptotic theory,Model averaging,Model selection,Plug-in estimators},
number = {1},
pages = {142--159},
title = {{Distribution theory of the least squares averaging estimator}},
volume = {186},
year = {2015}
}
@article{Otsu1993,
author = {Otsu, Taisuke and Rai, Yoshiyasu},
doi = {10.1080/01621459.2016.1231613},
file = {:Users/dagniel/Projects/ma/refs/Bootstrap inference of matching estimators for average treatment effects.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
title = {{Bootstrap inference of matching estimators for average treatment effects}},
year = {2016}
}
@article{Bickel2008,
abstract = {Abstract: For iid samples of size , the ordinary bootstrap (Efron (1979)) is known to be consistent in many situations, but it may fail in important examples ( Bickel , G{\"{o}}tze and van Zwet (1997)). Using bootstrap samples of size , where and , typically resolves the problem ( Bickel et al. ( ...},
author = {Bickel, Peter J. and Sakov, Anat},
file = {:Users/dagniel/Projects/ma/refs/BS2008SS.pdf:pdf},
isbn = {1017-0405},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {adaptive choice,and phrases,bootstrap,choice of m,data-dependent,extrema,m out of n,rule},
pages = {967--985},
title = {{On the choice of m in the m out of n bootstrap and confidence bounds for extrema}},
volume = {18},
year = {2008}
}
@Article{matching,
    title = {Multivariate and Propensity Score Matching Software with
      Automated Balance Optimization: The {Matching} Package for {R}},
    author = {Jasjeet S. Sekhon},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {42},
    number = {7},
    pages = {1--52},
    url = {http://www.jstatsoft.org/v42/i07/},
  }
@article{Chan2016,
abstract = {The estimation of average treatment effects based on observational data is extremely important in practice and has been studied by generations of statisticians under different frameworks. Existing globally efficient estimators require non-parametric estimation of a propensity score function, an outcome regression function or both, but their performance can be poor in practical sample sizes. Without explicitly estimating either function, we consider a wide class of calibration weights constructed to attain an exact three-way balance of the moments of observed covariates among the treated, the control and the combined group. The wide class includes exponential tilting, empirical likelihood and generalized regression as important special cases, and extends survey calibration estimators to different statistical problems and with important distinctions. Global semiparametric efficiency for the estimation of average treatment effects is established for this general class of calibration estimators. The results show that efficiency can be achieved by solely balancing the covariate distributions without resorting to direct estimation of the propensity score or outcome regression function. We also propose a consistent estimator for the efficient asymptotic variance, which does not involve additional functional estimation of either the propensity score or the outcome regression functions. The variance estimator proposed outperforms existing estimators that require a direct approximation of the efficient influence function.},
author = {Chan, Kwun Chuen Gary and Yam, Sheung Chi Phillip and Zhang, Zheng},
doi = {10.1111/rssb.12129},
file = {:Users/dagniel/Projects/stat{\_}group/ATE{\_}long.pdf:pdf},
isbn = {1369-7412},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Global semiparametric efficiency,Propensity score,Sieve estimator,Treatment effects},
mendeley-groups = {rand{\_}reading{\_}group},
number = {3},
pages = {673--700},
title = {{Globally efficient non-parametric inference of average treatment effects by empirical balancing calibration weighting}},
volume = {78},
year = {2016}
}
@article{Athey2016,
abstract = {There are many settings where researchers are interested in estimating average treatment effects and are willing to rely on the unconfoundedness assumption, which requires that the treatment assignment be as good as random conditional on pre-treatment variables. The unconfoundedness assumption is often more plausible if a large number of pre-treatment variables are included in the analysis, but this can worsen the performance of standard approaches to treatment effect estimation. In this paper, we develop a method for de-biasing penalized regression adjustments to allow sparse regression methods like the lasso to be used for sqrt{\{}n{\}}-consistent inference of average treatment effects in high-dimensional linear models. Given linearity, we do not need to assume that the treatment propensities are estimable, or that the average treatment effect is a sparse contrast of the outcome model parameters. Rather, in addition standard assumptions used to make lasso regression on the outcome model consistent under 1-norm error, we only require overlap, i.e., that the propensity score be uniformly bounded away from 0 and 1. Procedurally, our method combines balancing weights with a regularized regression adjustment.},
archivePrefix = {arXiv},
arxivId = {1604.07125},
author = {Athey, Susan and Imbens, Guido W. and Wager, Stefan},
eprint = {1604.07125},
file = {:Users/dagniel/Projects/ma/refs/athey{\_}imbens.pdf:pdf},
keywords = {causal inference,potential outcomes,propensity score,sparse estimation},
mendeley-groups = {model averaging},
number = {August},
pages = {1--28},
title = {{Approximate Residual Balancing: De-Biased Inference of Average Treatment Effects in High Dimensions}},
url = {http://arxiv.org/abs/1604.07125},
year = {2016}
}
@article{Iacus2009,
abstract = {This program is designed to improve causal inference via a method of matching that is widely applicable in observational data and easy to understand and use (if you understand how to draw a histogram, you will understand this method). The program implements the coarsened exact matching (CEM) algorithm, described below. CEM may be used alone or in combination with any existing matching method. This algorithm, and its statistical properties, are described in Iacus, King, and Porro (2008).},
author = {Iacus, Stefano M. and King, Gary and Porro, Giuseppe},
doi = {10.18637/jss.v030.i09},
file = {:Users/dagniel/Projects/ma/refs/cem.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {causal inference,matching,treatment effect estimation},
mendeley-groups = {model averaging},
number = {9},
title = {{{\textless}b{\textgreater}cem{\textless}/b{\textgreater} : Software for Coarsened Exact Matching}},
url = {http://www.jstatsoft.org/v30/i09/},
volume = {30},
year = {2009}
}


@article{Austin2009,
abstract = {Propensity-score matching is increasingly being used to reduce the$\backslash$nimpact of treatment-selection bias when estimating causal treatment$\backslash$neffects using observational data. Several propensity-score matching$\backslash$nmethods are currently employed in the medical literature: matching$\backslash$non the logit of the propensity score using calipers of width either$\backslash$n0.2 or 0.6 of the standard deviation of the logit of the propensity$\backslash$nscore; matching on the propensity score using calipers of 0.005,$\backslash$n0.01, 0.02, 0.03, and 0.1; and 5 → 1 digit matching on the propensity$\backslash$nscore. We conducted empirical investigations and Monte Carlo simulations$\backslash$nto investigate the relative performance of these competing methods.$\backslash$nUsing a large sample of patients hospitalized with a heart attack$\backslash$nand with exposure being receipt of a statin prescription at hospital$\backslash$ndischarge, we found that the 8 different methods produced propensity-score$\backslash$nmatched samples in which qualitatively equivalent balance in measured$\backslash$nbaseline variables was achieved between treated and untreated subjects.$\backslash$nSeven of the 8 propensity-score matched samples resulted in qualitatively$\backslash$nsimilar estimates of the reduction in mortality due to statin exposure.$\backslash$n5 → 1 digit matching resulted in a qualitatively different estimate$\backslash$nof relative risk reduction compared to the other 7 methods. Using$\backslash$nMonte Carlo simulations, we found that matching using calipers of$\backslash$nwidth of 0.2 of the standard deviation of the logit of the propensity$\backslash$nscore and the use of calipers of width 0.02 and 0.03 tended to have$\backslash$nsuperior performance for estimating treatment effects ({\textcopyright} 2009 WILEY-VCH$\backslash$nVerlag GmbH {\&} Co. KGaA, Weinheim)},
author = {Austin, Peter C.},
doi = {10.1002/bimj.200810488},
file = {:Users/dagniel/Projects/ma/refs/Austin 2009 Biometrical J.pdf:pdf},
isbn = {1521-4036},
issn = {03233847},
journal = {Biometrical Journal},
keywords = {Acute myocardial infarction,Balance,Bias,Matching,Monte Carlo simulations,Observational studies,Propensity score,Propensity-score matching},
number = {1},
pages = {171--184},
pmid = {19197955},
title = {{Some methods of propensity-score matching had superior performance to others: results of an empirical investigation and monte carlo simulations}},
volume = {51},
year = {2009}
}
@article{Politis2008,
author = {Politis, Dimitris N. and Romano, Joseph P.},
file = {:Users/dagniel/Projects/ma/refs/2242497.pdf:pdf},
journal = {Statistics},
number = {4},
pages = {2031--2050},
title = {{Large Sample Confidence Regions Based on Subsamples under Minimal Assumptions Author ( s ): Dimitris N . Politis and Joseph P . Romano Source : The Annals of Statistics , Vol . 22 , No . 4 , ( Dec ., 1994 ), pp . 2031-2050 Published by : Institute of Math}},
volume = {22},
year = {2008}
}
@article{Marron1991,
abstract = {A general weak convergence theory is developed for time-sequential censored rank statistics in the two-sample problem of comparing time to failure between two treatment groups, such as in the case of a clinical trial in which patients enter serially and, after being randomly allocated to one of two treatments, are followed until they fail or withdraw from the study or until the study is terminated. Applications of the theory to time-sequential tests based on these censored rank statistics are also discussed.},
author = {Marron, J. S. and Wand, M. P.},
file = {:Users/dagniel/Projects/ma/refs/euclid.aos.1176325770.pdf:pdf},
journal = {Annals of Statistics},
number = {3},
pages = {1403--1433},
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Statistics. {\textregistered} www.jstor.org}},
volume = {19},
year = {1991}
}
@article{Bodory2016,
author = {Bodory, Hugo and Huber, Martin and Bodory, Hugo and Camponovo, Lorenzo and Huber, Martin and Lechner, Michael},
file = {:Users/dagniel/Library/Application Support/Mendeley Desktop/Downloaded/Bodory et al. - 2016 - A Wild Bootstrap Algorithm for Propensity Score Matching Estimators A Wild Bootstrap Algorithm for Propensity Sco.pdf:pdf},
number = {June},
title = {{A Wild Bootstrap Algorithm for Propensity Score Matching Estimators A Wild Bootstrap Algorithm for Propensity Score Matching Estimators}},
year = {2016}
}
@article{Wang2009,
abstract = {Abstract  In applications, the traditional estimation procedure generally begins with model selection. Once a specific model is selected, subsequent estimation is conducted under the selected model without consideration of the uncertainty from the selection process. This often leads to the underreporting of variability and too optimistic confidence sets. Model averaging estimation is an alternative to this procedure, which incorporates model uncertainty into the estimation process. In recent years, there has been a rising interest in model averaging from the frequentist perspective, and some important progresses have been made. In this paper, the theory and methods on frequentist model averaging estimation are surveyed. Some future research topics are also discussed.},
author = {Wang, Haiying and Zhang, Xinyu and Zou, Guohua},
doi = {10.1007/s11424-009-9198-y},
file = {:Users/dagniel/Projects/ma/refs/7df9420c7604c9222db14ca58fbc2c5c56a8.pdf:pdf},
issn = {1009-6124},
journal = {Journal of Systems Science and Complexity},
keywords = {adaptive regression,asymptotic theory,frequentist model averaging,model selection},
number = {September},
pages = {732--748},
title = {{Frequentist model averaging estimation: a review}},
volume = {22},
year = {2009}
}
@article{Politis2001,
abstract = {A general approach to constructing confidence intervals by subsampling$\backslash$nwas presented in Politis and Romano (1994). The crux of the method$\backslash$nis recomputing a statistic over subsamples of the data, and these$\backslash$nrecomputed values are used to build up an estimated sampling distribution.$\backslash$nThe method works under extremely weak conditions, it applies to independent,$\backslash$nidentically distributed (i.i.d.) observations as well as to dependent$\backslash$ndata situations, such as time series (possibly nonstationary), random$\backslash$nfields, and marked point processes. In this article, we present some$\backslash$ntheorems showing: a new construction for confidence intervals that$\backslash$nremoves a previous condition, a general theorem showing the validity$\backslash$nof subsampling for data-dependent choices of the block size, and$\backslash$na general theorem for the construction of hypothesis tests (not necessarily$\backslash$nderived from a confidence interval construction). The arguments apply$\backslash$nto both the i.i.d. setting and the dependent data case.},
author = {Politis, D N and Romano, J P and Wolf, Michael},
file = {:Users/dagniel/Projects/ma/refs/a11n49.pdf:pdf},
isbn = {1017-0405},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,confidence intervals,data-dependent block size choice,hypothesis tests,large sample theory,resampling},
pages = {1105--1124},
title = {{On The Asymptotic Theory of Subsampling}},
volume = {11},
year = {2001}
}
@article{Liang2011,
abstract = {There has been increasing interest recently inmodel averagingwithin the frequentist paradigm. Themain benefit ofmodel averaging overmodel selection is that it incorporates rather than ignores the uncertainty inherent in themodel selection process. One of themost important, yet challenging, aspects of model averaging is how to optimally combine estimates from different models. In this work, we suggest a procedure of weight choice for frequentist model average estimators that exhibits optimality properties with respect to the estimatorsmean squared error (MSE). As a basis for demonstrating our idea, we consider averaging over a sequence of linear regression models. Building on this base, we develop a model weighting mechanism that involves minimizing the trace of an unbiased estimator of the model average estimators MSE. We further obtain results that reflect the finite sample aswell as asymptotic optimality of the proposedmechanism. AMonte Carlo study based on simulated and real data evaluates and compares the finite sample properties of thismechanism with those of existingmethods. The extension of the proposed weight selection scheme to general likelihood models is also considered. This article has supplementary material online.},
author = {Liang, Hua and Zou, Guohua and Wan, Alan T. K. and Zhang, Xinyu},
doi = {10.1198/jasa.2011.tm09478},
file = {:Users/dagniel/Projects/ma/refs/Optimal Weight Choice for Frequentist Model Average Estimators.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Asymptotic optimality, Finite sample properties, M,asymptotic optimality,finite sample properties,mallows criterion,smoothed aic,smoothed bic,unbiased mse},
number = {495},
pages = {1053--1066},
title = {{Optimal Weight Choice for Frequentist Model Average Estimators}},
url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2011.tm09478},
volume = {106},
year = {2011}
}
